---
title: "Assignment 4.5"
author: "Cory J. Cascalheira"
date: "03/13/2021"
output: 
  word_document:
    reference_docx: style_reference.docx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

<br>

# Assignment Instructions

Please complete the following:

1. Conduct a linear regression model for the following: PV1MATH = Intercept + ESCS\*X1 + Gender\*X2 + LANG\*X3 + ENJOYMATH\*X4

2. Install the package ppcor:

    * use the function spcor; type ?spcor to see how it works.

    * run it like this and store it into a variable such as the name "part" (because this function is called a part correlation, or also a "semi-part" correlation, hence the function spcor): part <- spcor(data[,c(column numbers of variables you are including in the model separated by columns, starting with the outcome measure and proceeding in the same order as you include the variables in the regression model), method = "pearson"]

        * Note: if you run the word "part" to get the full output, you only care about the $estimate section, AND, you are only interested in the first row, which shows the part/semi-part correlations. So, the second column, first row, will be the part/semi-part correlation for the first IV you included in your model (if you put them in the same order), and so on across the row.

    * now you can call the correlation and square it to get the unique proportion of variance that each  independent variable is contributing (little r2) to the overall proportion of variance (multiple R2) by squaring the part correlation values: part$estimate^2 (again, you are looking only at the first row, so you could do part$estimate[1,]^2 to only output the first row of values).

3. Include interpretation of the collinearity diagnostics (VIF and Tolerance) and Part and partial correlations in your write up.

# Establish the Work Environment

Load all the dependencies and import the data.

```{r}
# Dependencies
library(car)
library(psych)
library(ppcor)
library(tidyverse)
library(rio)

# Import
pisa <- import("../data/2012 PISA multiple countries selected variables.sav") %>%
  as_tibble

# Construct the APA theme for plots
# Construct the APA theme
apa_theme <- theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(),
        plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12, family = "sans"),
        axis.text.x = element_text(color = "black"),
        axis.text.y = element_text(color = "black"),
        axis.title = element_text(face = "bold"))
```

## Recode Data

First, I recoded the data to facilicate coding and interpretation.

```{r}
# Rename pisa
pisa_1 <- pisa %>%
  rename(country = CNT, language = ST25Q01, enjoy_math = ST29Q04,
         gender = ST04Q01, math_career = ST48Q05, applied_math = ST76Q01,
         solve_equation = ST37Q05, math_score = PV1MATH, ses = ESCS)

# Recode dichotomous values
pisa_2 <- pisa_1 %>%
  mutate(
    # 0 = same; 1 = different
    language = recode(language, `1` = 0, `2` = 1),
    # 0 = male; 1 = female
    gender = recode(gender, `1` = 1, `2` = 0),
    # Recode enjoy math so increasing numbers mean increased enjoyment
    enjoy_math = recode(enjoy_math, `1` = 4, `2` = 3, `3` = 2, `4` = 1)
  )
pisa_2
```

## Assumption Checking

Other major assumptions (e.g., normality, linearity) were assumed met since we have worked with this dataset several times before.

### Collinearity

Collinearity is a subset of multicollinearity applied to two predictors (Field et al., 2012). Multicollinearity occurs when two or more predictors are highly correlated within a regression model. As values of collinearity increase, problems are introduced into the model, such as:

* unstable beta weights, which might fit the sample but not represent the population;
* less contribution to multiple correlation (i.e., *R*) and variance explained because collinear variables share too much common variance; and
* difficulty assessing which predictors are most important.

First, we need to specify the model.

```{r}
# Create the model
model <- lm(math_score ~ ses + gender + language + enjoy_math, data = pisa_2)
```

The variance inflation factor (VIF) is a measure of "whether a predictor has a strong linear relationship with other predictor(s)" (Field et al., 2012, p. 276). Values of 10 are problematic. If the average VIF is greater than 1, then multicollinearity is likely a problem. 

Tolerance, according to Field and colleagues, is the reciprocal of VIF. Problems occurs when tolerance is less than 0.1 and multicollinearity is more likely a problem when tolerance is less than 0.2.

```{r}
# Check the variance inflation factor (VIF)
vif(model)

# Average VIF
mean(vif(model))

# Tolerance
1 / vif(model)
```

None of the above values reached the thresholds suggested by Field et al. (2012), therefore multicollinearity is likely not a problem. The average VIF is barely greater than 1.

## Part Correlations

A part correlation (i.e., a semi-partial correlation) is the *unique* correlation after overlapping association between two variables is removed. Conceptually, the procedure carves out a "part" of the multiple correlation coefficient; when the part correlation is squared, it carves out the variance explained (i.e., the multiple R^2) by the unique predictor.

Partial is like a relative frequency. It is out of 100% of multiple R^2. 

Whereas the part correlation carves out a part of the multiple R^2. For example, if the R^2 is 0.241 and the part correlation for SES is .483, then the squared value of the part correlation is 0.231, which is in the same units as multiple R^2. 

```{r}
# Select the variables in the model
pisa_3 <- pisa_2 %>%
  select(-country, -math_career, -applied_math, -solve_equation)

# Calculate semi-partial correlation (i.e., part)
# Remove missing values
part <- spcor(na.omit(as.data.frame(pisa_3)), method = "pearson")
part
```

# Interpretation

```{r}
# Summarize the model
summary(model)

# Confidence intervals
confint(model)

# Partial correlations - ses
round(part$estimate[2,1]^2, 4) * 100

# Partial correlations - language
round(part$estimate[3,1]^2, 4) * 100

# Partial correlations - enjoy_math
round(part$estimate[4,1]^2, 4) * 100

# Partial correlations - gender
round(part$estimate[5,1]^2, 4) * 100
```

Results from the multiple regression were significant, *F*(4, 41,676) = 3,304, *p* < .001, and the model explained 24.07% of the variance in math achievement. All of the variables were significant predictors. Socioeconomic status (*t* = 113.05, *p* < .001) significantly contributed to the variance in math achievement, indicating that for every one unit increase in standardized socioeconomic status, a student's score on the math achievement assessment would increase by 36.5 points (95% CI [35.87, 37.13]). Gender (*t* = 9.08, *p* < .001) and language (*t* = 10.06, *p* < .001) were also significant predictors, but the practical significance was limited: being female (versus male) was associated with a 7.28 point increase in math achievement (95% CI [5.71, 8.85]) and having a native language that was different from the test (versus the same) was associated with a 10.06 point increase in math achievement (95% CI [7.39, 12.73]). Finally, enjoyment of math exhibited a significant relationship with math achievement (*t* = 16.15, *p* < .001), indicating that participants tended to score 7.15 points higher (95% CI [6.28, 8.05]) for every one unit increase in math enjoyment. Again, math enjoyment is not considered practically significant given the scale of math achievement. 

Part correlations were conducted to examine the unique contributions to the overall variance in math scores. Part correlations were squared to determine the unique variance. Results indicated that standardized socioeconomic status explained `r round(part$estimate[2,1]^2, 4) * 100`% of the variance in math achievement, speaking a language different from the test explained `r round(part$estimate[3,1]^2, 4) * 100`%, student enjoyment of math explained `r round(part$estimate[4,1]^2, 4) * 100`%, and being female explained `r round(part$estimate[5,1]^2, 4) * 100`%. Therefore, only socioeconomic status explained a meaningful amount of the variance in math achievement. 

# References

Field, A., J. Miles, & Z. Field. (2012). *Discovering statistics using R*. SAGE Publications Ltd.
